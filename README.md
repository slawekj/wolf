#Wolf
Trade foreign exchange with ease.

### Introduction
Wolf Trading Platform is an interactive platform that supports real-time financial data visualization, as well as historical data lookup. It executes simple trading rules in real time and is simple to integrate with the brokerage services. It is currently available [here](http://ec2-54-183-118-188.us-west-1.compute.amazonaws.com/wolf/graph.new). You might have to wait a couple of seconds for the system to initialize. This interface was tested on Google Chrome Version 35.0.1916.153.

Below is an overview of the architecture of Wolf:

![alt tag](https://raw.githubusercontent.com/slawekj/wolf/master/images/architecture.png "Architecture of Wolf")

Wolf uses the following modules:

  1. Data.provider is responsible for a real-time Forex feed to the system. To have that free of charge it uses data aggregated by [Hist Data](histdata.com) every month. The data feed is therefore one month old, e.g., a tick that happened in June at exactly '2014-06-03 15:32:21.451 EST' is fed to the system on July at exactly '2014-07-03 15:32:21.451 EST'. This approach helps generating traffic similar to what expensive data providers would provide. The downside is, apart from data being one-month old, the fact that a regular weekday in July might have been a weekend in June, which causes outage in feed.
  2. Restful.rule.submission is responsible for getting user-defined orders from investors. They are defined as simple IF THEN trading rues. Rules are delivered to the rule.engine and executed if the condition specified in the order is met. All rules enter the system via this API. Web.interface calls this API when user "clicks" to submit a rule.
  3. Data.router is responsible for getting events generated by producers: "ticks" from data.provider and orders from restful.rule.submission and deliver them to the appropriate consumers. It is built on top of [Kafka](https://kafka.apache.org/) distributed queue.
  4. Rule.engine is one of the consumers of events provided by data.router. It matches up rules with the current state of the market and executes the trades when the conditions specified in rules are met. It is built on top of [Storm](http://storm.incubator.apache.org/) event processor. The design of rule.engine is showed below: ![alt tag](https://raw.githubusercontent.com/slawekj/wolf/master/images/engine.png "Architecture of Rule Engine") There are two spouts: one provides ticks, one provides rules. It is a simple design, which matches up rules with ticks by the "symbol" field, e.g. it uses Storm field grouping. Execution bolt stores partially the latest state of the market, e.g. each task "knows" only about the state of the input ticks.

  5. Data.aggregator.rt is responsible for aggregating the latest ticks in real time. It is built on top of [Cassandra](http://cassandra.apache.org/) database. Ticks are only retained for 3 hours. Data.aggreator takes advantage of dynamic column field, which is a mechanism of storing temporal time-series data in Cassandra.
  6. Data.aggregator.batch is responsible for aggregating a wide time range of ticks in batch, averaging them, and serving aggregated views to data.aggregator.rt. It is built on top of [HDFS](http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html), [Camus](https://github.com/linkedin/camus), and [Hive](https://hive.apache.org/). Ticks are stored in HDFS every 10 minutes by Camus data collector. Camus creates a set of compressed files, groups them by hours, each line in such a file is one serialized json object representing a tick. A Hive table is defined over the directory that contains the files from the last hour, files are decompressed on-the-fly, json objects get flatten, average prices within every minute are calculated and sent to data.aggregator.rt.
  7. Restful.cache.service.rt separates queries to the data.aggregator.rt database from clients. It uses in-memory cache to serve requests in real time.
  8. Restful.cache.service.batch works exactly like restful.cache.service.rt but for the other types of queries. The distinction have been made between these modules because they are likely to be queried at different rates, e.g. restful.cache.service.rt is queried more frequently to give user the best possible experience, whereas restful.cache.service.batch is queried less frequently because it depends on the data from data.aggregator.batch, served every 10 minutes or so.
  9. Web.interface is a front-end that a regular investor can use to analyze Forex market and order trades. It is built on top of [Flot](http://www.flotcharts.org/) JavaScript graphing library. 

### Getting started

I am currently working on the deployment scripts, all the modules should be easy to deploy using the following commands:
```
git clone https://github.com/slawekj/wolf.git
./wolf/<module name>/bin/install.sh
./wolf/<module name>/bin/run.sh
```
Modules should be deployed in the following order:
  1. data.router (deployment scripts ready)
  2. data.provider (deployment scripts ready)
  3. rule.engine (deployment scripts in preparation)
  4. data.aggregator.rt (deployment scripts in preparation)
  5. data.aggregator.batch (deployment scripts in preparation)
  6. restful.cache.rt (deployment scripts in preparation)
  7. restful.cache.batch (deployment scripts in preparation)
  8. restful.rule.submission (deployment scripts in preparation)
  9. web.interface (deployment scripts in preparation)

Deployment scripts are tested on Ubuntu 12.04 distribution, which is available [here](http://releases.ubuntu.com/12.04/ubuntu-12.04.4-server-amd64.iso). You should have sudo permissions and have git installed:
```
sudo apt-get install git
```
